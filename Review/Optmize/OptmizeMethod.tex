\section{最优化方法}
下列方法主要用于解决提答题中的最优化问题\CJKsout{以及乱搞骗分}。
\subsection{爬山法}
\index{H!Hill Climbing}
每次迭代时选取与$x$点临近的点$x'$计算，若其值更优，则接受$x'$（也可以选择临近最优点）
，进入下一步迭代；若所有临近点都比$x$要差，那么此时$x$局部最优。重新随机选取初始点进行
迭代，将迭代过的点中最优解作为答案。
\subsubsection{随机爬山法}
\index{S!Stochastic Hill Climbing}
在选取下一迭代点时，还可以对更优点集按照变化值建立分布，然后离散随机采样。
\subsection{模拟退火}
\index{S!Simulated Annealing}
模拟退火为比赛中的常用方法。
步骤如下：
\begin{enumerate}
	\item 选取一个初始温度$T$，与冷却系数$d\in (0,1)$，初始化一个解$s$，计算$f(s)$；
	\item 稍微修改该解得到新解$s'$，计算$f(s')$；
	\item \begin{itemize}
		      \item 如果$f(s')>f(s)$，则直接接受新解$s'$；
		      \item 否则按照Metropolis准则，以$e^{-\frac{\Delta f}{T}}$的概率接受新解。
	      \end{itemize}
	\item 冷却，即$T\leftarrow Td$;
	\item 当$T$足够小时退出，否则返回步骤2。
\end{enumerate}
$T$与$d$的作用是让变化越来越``保守''。在整个模拟退火的过程中要另外维护一个搜索到
的最优解，以增加正确率。

\subsubsection{参数选取}
参数$T$与$d$的选取是玄学。。。可以结合从某解到最优解的最大/期望/最小步数考虑。

Update：在FlashHu的博客里找到了人肉调参方法\footnote{
	模拟退火总结（模拟退火）\\
	\url{https://www.cnblogs.com/flashhu/p/8884132.html}
}：调参时二分初始温度与冷却系数（主要调冷却系数），每个二分值测试多次（因为随机化）。
迭代输出当前解与对应温度，\CJKsout{用心}感受解的下降速度是否均匀，如果下降速度太快或
太慢就调整对应时间段内的冷却系数。或许有自动化的算法完成这一过程？启发式模拟退火？

\index{*TODO!启发式模拟退火}
\subsubsection{新解生成}
注意在计算新解的价值时，尽量由旧解的价值修改得出，以降低更新解的复杂度。
在变化时要尽量避免价值差变化过大（比如TSP中交换相邻城市比交换任意两个城市更好）。
当然为了防止解在一个``盆地''中过分逗留，可以重新选取初始解或者选择恰当时机
对当前解进行比较大的改变。随着温度的减小，对解的变动也应该减小。
\subsubsection{分块模拟退火}
对于多峰函数，将区间分成几块，在块内使用模拟退火（因为模拟退火利用相邻解的关系）。

以上内容参考了Wikipeida-EN\footnote{
	Simulated annealing - Wikipedia\\
	\url{https://en.wikipedia.org/wiki/Simulated\_annealing}
}
\subsection{遗传算法}
\index{G!Genetic Algorithm}
遗传算法引入了生物学中的概念：遗传、变异、竞争与淘汰。

算法步骤如下：
\begin{enumerate}
	\item 生成一些初始解，计算这些解的``适应度''；
	\item 选取这些解的一部分较优解；
	\item 通过较优解之间的配对交叉把自己的``基因''（编码）遗传给
		  子代（可以加权），子代的基因以一定的概率变异（只要考虑$p<0.5$），
		  加入下一代群体；
	\item 迭代固定次数后退出，否则返回步骤2；
	\item 输出计算历史中适应度最高的解。
\end{enumerate}
\subsection{A*}
A*算法通过引入一个估价函数$h(x)$表示到目标点长度的估计值，
在做最短路时使用$f(x)+h(x)$作为权重更新距离，可以达到比Dijkstra
更好的性能。
\subsubsection{IDA*}
IDA*比A*多了一个迭代加深的操作，即每次DFS/BFS搜索时限制搜索深度。
若使用当前深度参数无法找到可行解，就加深深度重新DFS，注意这里
不一定要保存上一次的搜索树，因为新搜索树比原搜索树大得多。
\subsection{梯度下降}
\index{G!Gradient Descent}
梯度下降法用来找局部最优值，其主要思想是利用当前点的梯度来引导寻找更优值。
梯度在笛卡尔坐标系中被定义为一个向量，其坐标值为目标函数在对应基向量上的偏导数。
若要寻找$f(x)$最小值，迭代时令$x_{n+1}=x_n-\gamma \nabla f(x_n)$。

注意$\gamma$要小且不需要更改，因为随着算法的收敛，$\nabla f(x)$会越来越小。
迭代时达到指定精度或者固定迭代次数后直接退出，注意要不断更换$\gamma$来
保证算法的收敛（$\gamma$过大会导致波动幅度大，过小会导致收敛速度慢）。

\paragraph{Barzilai-Borwein Method}
\index{B!Barzilai-Borwein Method}
这是一种自适应设置步长$\gamma$的方法，其表达式如下：
\begin{eqnarray*}
	D&=&\nabla f(x_n)-\nabla f(x_{n-1})\\
	\gamma_n&=&(x_n-x_{n-1})^T\cdot \hat{D}\\
\end{eqnarray*}

$\hat{D}$表示$D$对应的单位向量。

例如求$f(x)=x^4-3x^3+2$的在$x_0=6$附近的最小值所对应的$x$：

\lstinputlisting{Optmize/GD.cpp}

上述内容参考了Wikipedia-EN\footnote{
	Gradient descent - Wikipedia
	\url{https://en.wikipedia.org/wiki/Gradient\_descent}
}。
\subsection{解集存储}
在解决最优化问题时，保存新解{\bfseries 当且仅当该解比存储的最优解更优且
该解即将被比自己更差的解作为当前解}，在迭代退出再执行一遍该逻辑。此法延迟了解集保存时间，
可以减少冗余拷贝。Lazy Evaluation大法好！！！
\subsection{拉格朗日乘子法}
\index{L!Lagrange Multiplier}
拉格朗日乘子法用于求解{\bfseries 多元函数条件极值问题}。
给定两个一阶连续可导函数$f(X),g(X)$，$X$为变量组，在满足$g(X)=0$的条件下最优化$f(X)$。
也存在$g(X)=c$的形式，常数一般不隐藏在$g(X)$中。

拉格朗日乘子法引入了一个新的变量$\lambda$，称作拉格朗日乘子。然后构造一个新的
函数$\mathcal{L}(X,\lambda)=f(X)-\lambda g(X)$，称作拉格朗日函数。

我们在$g(X)=0$的区域上移动，如果在某处$f(X)$局部不改变的话，这个地方很有可能取得极值，
且极值点只可能在这些点取到。考虑``等势区域''$g(X)=0$与$f(X)=d$，$f(X)$的值不改变意味着
它也在等势区域上移动。那么有这两个等势区域在该处平行，意味着这个$X$在$f(X)$与$g(X)$上的
梯度平行，记为$\nabla_X f=\lambda \nabla_X g$。再加上约束$g(X)=0$，结合先前引入的
拉格朗日函数，有$\nabla_{X,\lambda} \mathcal{L}=0$。

在实际计算中，首先根据$\nabla_X \mathcal{L}=0$用$\lambda$表达出$X$，然后代入$g(X)=0$
求出可行的$\lambda$。这时解的搜索范围被极大地缩小，直接比较每个$\lambda$对应的$f(X)$，
得到所需极值。多个$g(X)=0$的处理方法类似。

\paragraph{例题} 「NOI2012」骑行川藏

很容易将其转化为多元函数条件极值问题：
\begin{itemize}
	\item 满足约束$\displaystyle g(V)=\sum{k_i(v_i-v_i')^2s_i}=E_U$
	\item 最小化$\displaystyle f(V)=\sum{\frac{s_i}{v_i}}$
\end{itemize}

求偏导得$\nabla_{v_i} \mathcal{L}=-s_iv_i^{-2}-2\lambda s_ik_i(v_i-v_i')$，
令其为0，化简得$2\lambda k_i(v_i-v_i')v_i^2+1=0$。注意到$\lambda$为负数，
$\lambda$越大，每个$v_i$越大，耗费的能量$g(V)$越大。

那么可以二分$\lambda$，根据式子使用牛顿迭代法求出$v_i$，回代求出$g(V)$，与$E_U$比较。

一个比较trick的方法是以上一次迭代的解为初始解，当$\delta<\varepsilon$时退出迭代，
得到不错的性能提升。\CJKsout{对着数据调参数}

参考代码：
\lstinputlisting{Source/Source/Optmize/LOJ2671.cpp}

\subsubsection{KKT条件}
\index{K!Karush–Kuhn–Tucker\\ Conditions}
KKT条件是拉格朗日乘子法的扩展，用于求解不等式约束下的最优化问题。

看上去不太好求解，留坑待补。
\index{*TODO!KKT条件}
