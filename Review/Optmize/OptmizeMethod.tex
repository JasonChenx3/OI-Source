\section{最优化方法}
下列方法主要用于解决提答题中的最优化问题\sout{以及乱搞骗分}。
\subsection{爬山法}
\index{H!Hill Climbing}
每次迭代时选取与$x$点临近的点$x'$计算，若其值更优，则接受$x'$（也可以选择临近最优点）
，进入下一步迭代；若所有临近点都比$x$要差，那么此时$x$局部最优。重新随机选取初始点进行
迭代，将迭代过的点中最优解作为答案。
\subsubsection{随机爬山法}
\index{S!Stochastic Hill Climbing}
在选取下一迭代点时，还可以对更优点集按照变化值建立分布，然后离散随机采样。
\subsection{模拟退火}
\index{S!Simulated Annealing}
模拟退火为比赛中的常用方法。
步骤如下：
\begin{enumerate}
	\item 选取一个初始温度$T$，与冷却系数$d\in (0,1)$，初始化一个解$s$，计算$f(s)$；
	\item 稍微修改该解得到新解$s'$，计算$f(s')$；
	\item \begin{itemize}
		      \item 如果$f(s')>f(s)$，则直接接受新解$s'$；
		      \item 否则按照Metropolis准则，以$e^{-\frac{\Delta f}{T}}$的概率接受新解。
	      \end{itemize}
	\item 冷却，即$T\leftarrow Td$;
	\item 当$T$足够小时退出，否则返回步骤2。
\end{enumerate}
$T$与$d$的作用是让变化越来越``保守''。在整个模拟退火的过程中要另外维护一个搜索到
的最优解，以增加正确率。

\subsubsection{参数选取}
参数$T$与$d$的选取是玄学。。。可以结合从某解到最优解的最大/期望/最小步数考虑。
\subsubsection{新解生成}
注意在计算新解的价值时，尽量由旧解的价值修改得出，以降低更新解的复杂度。
在变化时要尽量避免价值差变化过大（比如TSP中交换相邻城市比交换任意两个城市更好）。
当然为了防止解在一个``盆地''中过分逗留，可以重新选取初始解或者选择恰当时机
对当前解进行比较大的改变。随着温度的减小，对解的变动也应该减小。
\subsubsection{分块模拟退火}
对于多峰函数，将区间分成几块，在块内使用模拟退火（因为模拟退火利用相邻解的关系）。

以上内容参考了Wikipeida-EN\footnote{
	Simulated annealing - Wikipedia\\
	\url{https://en.wikipedia.org/wiki/Simulated\_annealing}
}
\subsection{遗传算法}
\index{G!Genetic Algorithm}
遗传算法引入了生物学中的概念：遗传、变异、竞争与淘汰。

算法步骤如下：
\begin{enumerate}
	\item 生成一些初始解，计算这些解的``适应度''；
	\item 选取这些解的一部分较优解；
	\item 通过较优解之间的配对交叉把自己的``基因''（编码）遗传给
	      子代（可以加权），同时对子代的基因进行适当变异，加入下一代群体；
	\item 迭代固定次数后退出，否则返回步骤2；
	\item 输出计算历史中适应度最高的解。
\end{enumerate}
\subsection{A*}
A*算法通过引入一个估价函数$h(x)$表示到目标点长度的估计值，
在做最短路时使用$f(x)+h(x)$作为权重更新距离，可以达到比Dijkstra
更好的性能。
\subsubsection{IDA*}
IDA*比A*多了一个迭代加深的操作，即每次DFS/BFS搜索时限制搜索深度。
若使用当前深度参数无法找到可行解，就加深深度重新DFS，注意这里
不一定要保存上一次的搜索树，因为新搜索树比原搜索树大得多。
\subsection{梯度下降}
\index{G!Gradient Descent}
梯度下降法用来找局部最优值，其主要思想是利用当前点的梯度来引导寻找更优值。
梯度在笛卡尔坐标系中被定义为一个向量，其坐标值为目标函数在对应基向量上的偏导数。
若要寻找$f(x)$最小值，迭代时令$x_{n+1}=x_n-\gamma \nabla f(x_n)$。

注意$\gamma$要小且不需要更改，因为随着算法的收敛，$\nabla f(x)$会越来越小。
迭代时达到指定精度或者固定迭代次数后直接退出，注意要不断更换$\gamma$来
保证算法的收敛（$\gamma$过大会导致波动幅度大，过小会导致收敛速度慢）。

\paragraph{Barzilai-Borwein Method}
\index{B!Barzilai-Borwein Method}
这是一种自适应设置步长$\gamma$的方法，其表达式如下：
\begin{eqnarray*}
	D&=&\nabla f(x_n)-\nabla f(x_{n-1})\\
	\gamma_n&=&(x_n-x_{n-1})^T\cdot \hat{D}\\
\end{eqnarray*}

例如求$f(x)=x^4-3x^3+2$的在$x_0=6$附近的最小值所对应的$x$：

\lstinputlisting{Optmize/GD.cpp}

上述内容参考了Wikipedia-EN\footnote{
	Gradient descent - Wikipedia
	\url{https://en.wikipedia.org/wiki/Gradient\_descent}
}。
\subsection{解集存储}
在解决最优化问题时，保存新解{\bfseries 当且仅当该解比存储的最优解更优且
该解即将被比自己更差的解作为当前解}，在迭代退出再执行一遍该逻辑。此法延迟了解集保存时间，
可以减少冗余拷贝。Lazy Evaluation大法好！！！
